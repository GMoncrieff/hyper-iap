# first we specify what we're sweeping
# we specify a program to run
program: train.py
# we optionally specify how to run it, including setting default arguments
command:  
    - ${env}
    - ${interpreter}
    - ${program}
    - "--wandb"
    - "--log_every_n_steps"
    - "50"
    - "--max_epochs_noisy"
    - "5"
    - "--max_epochs_clean"
    - "1000"
    - "--val_check_interval"
    - "1.0"
    - "--gradient_clip_val"
    - "0.5"
    - "--precision"
    - "16"
    - "--run_clean"
    - ${args}  # these arguments come from the sweep parameters below

# and we specify which parameters to sweep over, what we're optimizing, and how we want to optimize it
method: random  # generally, random searches perform well, can also be "grid" or "bayes"
metric:
    name: final_target
    goal: maximize
parameters:  
    ##########################
    # pipeline hyperparameters
    ###########################
    ls_modifier:
        values: [0.0, 0.2]
    ##########################
    # litmodel hyperparameters
    ###########################
    #suggest 0.006
    lr:
        values: [0.001, 0.01]
    lr_ft:
        values: [0.001, 0.0001]
    ###########################
    # CNN hyperparameters
    ###########################
    hidden_dim:
        values: [128,64,12]
    kernel_size:
        values: [3,5]
    dropout:
        values: [0, 0.2]
    # we can also fix some values, just like we set default arguments
    #gpus:
    #    value: 1
    model_class:
        value: tempcnn.TEMPCNN
    transfer_class:
        value: tempcnn.TransferLearningTempCNN
    data_class:
        value: xarray_module.XarrayDataModule
    ft_schedule:
        value: hyperiap/litmodels/LitClassifier_tempcnn_ft_schedule.yaml