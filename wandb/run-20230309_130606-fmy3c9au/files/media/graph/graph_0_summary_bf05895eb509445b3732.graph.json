{"format": "torch", "nodes": [{"name": "model", "id": 140352170787360, "class_name": "MAE(\n  (encoder): simpleVIT(\n    (embedding): simpleVITextractor(\n      (pad): ReplicationPad1d((0, 1))\n      (to_patch_embedding): Sequential(\n        (0): Rearrange('b z (c p) -> b c (p z)', p=2)\n        (1): Linear(in_features=18, out_features=64, bias=True)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (transformer): Transformer(\n        (layers): ModuleList(\n          (0): ModuleList(\n            (0): Attention(\n              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n              (attend): Softmax(dim=-1)\n              (to_qkv): Linear(in_features=64, out_features=192, bias=False)\n              (to_out): Sequential(\n                (0): Linear(in_features=64, out_features=64, bias=True)\n                (1): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (1): FeedForward(\n              (net): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=8, bias=True)\n                (2): GELU(approximate='none')\n                (3): Dropout(p=0.3, inplace=False)\n                (4): Linear(in_features=8, out_features=64, bias=True)\n                (5): Dropout(p=0.3, inplace=False)\n              )\n            )\n          )\n          (1): ModuleList(\n            (0): Attention(\n              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n              (attend): Softmax(dim=-1)\n              (to_qkv): Linear(in_features=64, out_features=192, bias=False)\n              (to_out): Sequential(\n                (0): Linear(in_features=64, out_features=64, bias=True)\n                (1): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (1): FeedForward(\n              (net): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=8, bias=True)\n                (2): GELU(approximate='none')\n                (3): Dropout(p=0.3, inplace=False)\n                (4): Linear(in_features=8, out_features=64, bias=True)\n                (5): Dropout(p=0.3, inplace=False)\n              )\n            )\n          )\n          (2): ModuleList(\n            (0): Attention(\n              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n              (attend): Softmax(dim=-1)\n              (to_qkv): Linear(in_features=64, out_features=192, bias=False)\n              (to_out): Sequential(\n                (0): Linear(in_features=64, out_features=64, bias=True)\n                (1): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (1): FeedForward(\n              (net): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=8, bias=True)\n                (2): GELU(approximate='none')\n                (3): Dropout(p=0.3, inplace=False)\n                (4): Linear(in_features=8, out_features=64, bias=True)\n                (5): Dropout(p=0.3, inplace=False)\n              )\n            )\n          )\n          (3): ModuleList(\n            (0): Attention(\n              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n              (attend): Softmax(dim=-1)\n              (to_qkv): Linear(in_features=64, out_features=192, bias=False)\n              (to_out): Sequential(\n                (0): Linear(in_features=64, out_features=64, bias=True)\n                (1): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (1): FeedForward(\n              (net): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=8, bias=True)\n                (2): GELU(approximate='none')\n                (3): Dropout(p=0.3, inplace=False)\n                (4): Linear(in_features=8, out_features=64, bias=True)\n                (5): Dropout(p=0.3, inplace=False)\n              )\n            )\n          )\n          (4): ModuleList(\n            (0): Attention(\n              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n              (attend): Softmax(dim=-1)\n              (to_qkv): Linear(in_features=64, out_features=192, bias=False)\n              (to_out): Sequential(\n                (0): Linear(in_features=64, out_features=64, bias=True)\n                (1): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (1): FeedForward(\n              (net): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=8, bias=True)\n                (2): GELU(approximate='none')\n                (3): Dropout(p=0.3, inplace=False)\n                (4): Linear(in_features=8, out_features=64, bias=True)\n                (5): Dropout(p=0.3, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (to_latent): Identity()\n    )\n    (linear_head): Sequential(\n      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      (1): Linear(in_features=64, out_features=52, bias=True)\n    )\n  )\n  (pad): ReplicationPad1d((0, 1))\n  (to_patch): Rearrange('b z (c p) -> b c (p z)', p=2)\n  (patch_to_emb): Linear(in_features=18, out_features=64, bias=True)\n  (enc_to_dec): Linear(in_features=64, out_features=128, bias=True)\n  (decoder): Transformer(\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): Attention(\n          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (attend): Softmax(dim=-1)\n          (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n          (to_out): Sequential(\n            (0): Linear(in_features=128, out_features=128, bias=True)\n            (1): Dropout(p=0, inplace=False)\n          )\n        )\n        (1): FeedForward(\n          (net): Sequential(\n            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (1): Linear(in_features=128, out_features=512, bias=True)\n            (2): GELU(approximate='none')\n            (3): Dropout(p=0, inplace=False)\n            (4): Linear(in_features=512, out_features=128, bias=True)\n            (5): Dropout(p=0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (decoder_pos_emb): Embedding(134, 128)\n  (to_pixels): Linear(in_features=128, out_features=18, bias=True)\n)", "parameters": [["mask_token", [128]], ["encoder.embedding.pos_embedding", [1, 134, 64]], ["encoder.embedding.to_patch_embedding.1.weight", [64, 18]], ["encoder.embedding.to_patch_embedding.1.bias", [64]], ["encoder.embedding.transformer.layers.0.0.norm.weight", [64]], ["encoder.embedding.transformer.layers.0.0.norm.bias", [64]], ["encoder.embedding.transformer.layers.0.0.to_qkv.weight", [192, 64]], ["encoder.embedding.transformer.layers.0.0.to_out.0.weight", [64, 64]], ["encoder.embedding.transformer.layers.0.0.to_out.0.bias", [64]], ["encoder.embedding.transformer.layers.0.1.net.0.weight", [64]], ["encoder.embedding.transformer.layers.0.1.net.0.bias", [64]], ["encoder.embedding.transformer.layers.0.1.net.1.weight", [8, 64]], ["encoder.embedding.transformer.layers.0.1.net.1.bias", [8]], ["encoder.embedding.transformer.layers.0.1.net.4.weight", [64, 8]], ["encoder.embedding.transformer.layers.0.1.net.4.bias", [64]], ["encoder.embedding.transformer.layers.1.0.norm.weight", [64]], ["encoder.embedding.transformer.layers.1.0.norm.bias", [64]], ["encoder.embedding.transformer.layers.1.0.to_qkv.weight", [192, 64]], ["encoder.embedding.transformer.layers.1.0.to_out.0.weight", [64, 64]], ["encoder.embedding.transformer.layers.1.0.to_out.0.bias", [64]], ["encoder.embedding.transformer.layers.1.1.net.0.weight", [64]], ["encoder.embedding.transformer.layers.1.1.net.0.bias", [64]], ["encoder.embedding.transformer.layers.1.1.net.1.weight", [8, 64]], ["encoder.embedding.transformer.layers.1.1.net.1.bias", [8]], ["encoder.embedding.transformer.layers.1.1.net.4.weight", [64, 8]], ["encoder.embedding.transformer.layers.1.1.net.4.bias", [64]], ["encoder.embedding.transformer.layers.2.0.norm.weight", [64]], ["encoder.embedding.transformer.layers.2.0.norm.bias", [64]], ["encoder.embedding.transformer.layers.2.0.to_qkv.weight", [192, 64]], ["encoder.embedding.transformer.layers.2.0.to_out.0.weight", [64, 64]], ["encoder.embedding.transformer.layers.2.0.to_out.0.bias", [64]], ["encoder.embedding.transformer.layers.2.1.net.0.weight", [64]], ["encoder.embedding.transformer.layers.2.1.net.0.bias", [64]], ["encoder.embedding.transformer.layers.2.1.net.1.weight", [8, 64]], ["encoder.embedding.transformer.layers.2.1.net.1.bias", [8]], ["encoder.embedding.transformer.layers.2.1.net.4.weight", [64, 8]], ["encoder.embedding.transformer.layers.2.1.net.4.bias", [64]], ["encoder.embedding.transformer.layers.3.0.norm.weight", [64]], ["encoder.embedding.transformer.layers.3.0.norm.bias", [64]], ["encoder.embedding.transformer.layers.3.0.to_qkv.weight", [192, 64]], ["encoder.embedding.transformer.layers.3.0.to_out.0.weight", [64, 64]], ["encoder.embedding.transformer.layers.3.0.to_out.0.bias", [64]], ["encoder.embedding.transformer.layers.3.1.net.0.weight", [64]], ["encoder.embedding.transformer.layers.3.1.net.0.bias", [64]], ["encoder.embedding.transformer.layers.3.1.net.1.weight", [8, 64]], ["encoder.embedding.transformer.layers.3.1.net.1.bias", [8]], ["encoder.embedding.transformer.layers.3.1.net.4.weight", [64, 8]], ["encoder.embedding.transformer.layers.3.1.net.4.bias", [64]], ["encoder.embedding.transformer.layers.4.0.norm.weight", [64]], ["encoder.embedding.transformer.layers.4.0.norm.bias", [64]], ["encoder.embedding.transformer.layers.4.0.to_qkv.weight", [192, 64]], ["encoder.embedding.transformer.layers.4.0.to_out.0.weight", [64, 64]], ["encoder.embedding.transformer.layers.4.0.to_out.0.bias", [64]], ["encoder.embedding.transformer.layers.4.1.net.0.weight", [64]], ["encoder.embedding.transformer.layers.4.1.net.0.bias", [64]], ["encoder.embedding.transformer.layers.4.1.net.1.weight", [8, 64]], ["encoder.embedding.transformer.layers.4.1.net.1.bias", [8]], ["encoder.embedding.transformer.layers.4.1.net.4.weight", [64, 8]], ["encoder.embedding.transformer.layers.4.1.net.4.bias", [64]], ["encoder.linear_head.0.weight", [64]], ["encoder.linear_head.0.bias", [64]], ["encoder.linear_head.1.weight", [52, 64]], ["encoder.linear_head.1.bias", [52]], ["enc_to_dec.weight", [128, 64]], ["enc_to_dec.bias", [128]], ["decoder.layers.0.0.norm.weight", [128]], ["decoder.layers.0.0.norm.bias", [128]], ["decoder.layers.0.0.to_qkv.weight", [384, 128]], ["decoder.layers.0.0.to_out.0.weight", [128, 128]], ["decoder.layers.0.0.to_out.0.bias", [128]], ["decoder.layers.0.1.net.0.weight", [128]], ["decoder.layers.0.1.net.0.bias", [128]], ["decoder.layers.0.1.net.1.weight", [512, 128]], ["decoder.layers.0.1.net.1.bias", [512]], ["decoder.layers.0.1.net.4.weight", [128, 512]], ["decoder.layers.0.1.net.4.bias", [128]], ["decoder_pos_emb.weight", [134, 128]], ["to_pixels.weight", [18, 128]], ["to_pixels.bias", [18]]], "output_shape": [[100, 100, 18], [100, 100, 18]], "num_parameters": [128, 8576, 1152, 64, 64, 64, 12288, 4096, 64, 64, 64, 512, 8, 512, 64, 64, 64, 12288, 4096, 64, 64, 64, 512, 8, 512, 64, 64, 64, 12288, 4096, 64, 64, 64, 512, 8, 512, 64, 64, 64, 12288, 4096, 64, 64, 64, 512, 8, 512, 64, 64, 64, 12288, 4096, 64, 64, 64, 512, 8, 512, 64, 64, 64, 3328, 52, 8192, 128, 128, 128, 49152, 16384, 128, 128, 128, 65536, 512, 65536, 128, 17152, 2304, 18]}], "edges": []}